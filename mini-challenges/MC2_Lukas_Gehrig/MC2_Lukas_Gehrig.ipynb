{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of 39,181 images originating from people who are blind that are each paired with 5 captions. Our proposed challenge addresses the task of predicting a suitable caption given an image. Ultimately, we hope this work will educate more people about the technological needs of blind people while providing an exciting new opportunity for researchers to develop assistive technologies that eliminate accessibility barriers for blind people (https://vizwiz.org/tasks-and-datasets/image-captioning/).\n",
    "\n",
    "The goal of this Challenge is to create a single model similar to https://arxiv.org/pdf/1411.4555.pdf to get reasonable results on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#def extract_zip(filename):\n",
    "#    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#        zip_ref.extractall('./')\n",
    "#            \n",
    "#extract_zip('vizwiz_eval_cap.zip')\n",
    "\n",
    "#!pip3 install torch torchvision torchaudio\n",
    "#!pip install tensorboard\n",
    "#!pip install torchtext\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizwiz_api.vizwiz import VizWiz\n",
    "from vizwiz_eval_cap.eval import VizWizEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import Counter\n",
    "\n",
    "import json, os\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_alloc_dicts(set_name, vizwiz=None):\n",
    "    # be sure if `vizwiz` is set, that it contains the `set_name` dataset\n",
    "    if (set_name != 'train') and (set_name != 'val') and (set_name != 'test'):\n",
    "        raise Exception('only \"train\", \"val\" or \"test\" is a valid `set_name`')\n",
    "    \n",
    "    if not isinstance(vizwiz, VizWiz):\n",
    "        ann_path = './annotations/'+set_name+'.json'\n",
    "        vizwiz = VizWiz(ann_path, ignore_rejected=True, ignore_precanned=True)\n",
    "    \n",
    "    img_path_prefix = './images/'+set_name+'/'\n",
    "    img_ids_anns = np.unique([vizwiz.anns[i]['image_id'] for i in vizwiz.anns])\n",
    "    img_ids_imgs = np.unique([vizwiz.imgs[i]['id'] for i in vizwiz.imgs])\n",
    "    img_ids_with_capitions = np.array([_id for _id in img_ids_imgs if _id in img_ids_anns])\n",
    "    imgIdx_enumIdx = {imgIdx:idx for idx, imgIdx in enumerate(img_ids_with_capitions)}\n",
    "    \n",
    "    imgIdx_imgPath = {vizwiz.imgs[i]['id']:img_path_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs if vizwiz.imgs[i]['id'] in img_ids_with_capitions}\n",
    "    capIdx_imgIdx = {vizwiz.anns[i]['id']:vizwiz.anns[i]['image_id'] for i in vizwiz.anns}\n",
    "    enumIdx_capIdx = {idx:vizwiz.anns[i]['id'] for idx, i in enumerate(vizwiz.anns)}\n",
    "    capIdx_cap = {vizwiz.anns[i]['id']:vizwiz.anns[i]['caption'] for i in vizwiz.anns}\n",
    "    \n",
    "    def get_img_path(idx):\n",
    "        capIdx = enumIdx_capIdx[idx]\n",
    "        imgIdx = capIdx_imgIdx[capIdx]\n",
    "        imgPath = imgIdx_imgPath[imgIdx]\n",
    "        return imgPath\n",
    "        \n",
    "    return imgIdx_imgPath, capIdx_imgIdx, enumIdx_capIdx, capIdx_cap, get_img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train = './annotations/train.json'\n",
    "vizwiz_train = VizWiz(ann_train, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_train, capIdx_imgIdx_train, enumIdx_capIdx_train, capIdx_cap_train, get_img_path_train = get_alloc_dicts('train', vizwiz_train)\n",
    "\n",
    "ann_val = './annotations/val.json'\n",
    "vizwiz_val = VizWiz(ann_val, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_val, capIdx_imgIdx_val, enumIdx_capIdx_val, capIdx_cap_val, get_img_path_val = get_alloc_dicts('val', vizwiz_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1_idx = enumIdx_capIdx_train[0]\n",
    "cap1 = capIdx_cap_train[cap1_idx]\n",
    "print('caption:', cap1)\n",
    "img1_idx = capIdx_imgIdx_train[cap1_idx]\n",
    "img1 = Image.open(imgIdx_imgPath_train[img1_idx])\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "to_pil = transforms.Compose([transforms.ToPILImage()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_processed = preprocess(img1)\n",
    "img1_processed_pil = to_pil(img1_processed)\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1_processed_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(capIdx_cap_train, size=10000):\n",
    "    size -= 1 # -1 to address specials '<eos>'\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    counter = Counter()\n",
    "    for i in capIdx_cap_train:\n",
    "        sentence = capIdx_cap_train[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        counter.update(tokenizer(sentence.lower()))\n",
    "\n",
    "    top_tokens = list(dict(counter.most_common(size)).keys())\n",
    "    drop_tokens = list(set(counter.keys()) - set(top_tokens))\n",
    "    for drop_token in drop_tokens: # drops all `drop_tokens` from `corpus_counter`\n",
    "        counter.pop(drop_token)\n",
    "\n",
    "    vocab = Vocab(counter, specials=['<eos>'])\n",
    "    \n",
    "    return tokenizer, vocab\n",
    "\n",
    "tokenizer, vocab = build_vocab(capIdx_cap_train, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lengths_quantiles(capIdx_cap, tokenizer, vocab):\n",
    "    lengths = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # filters the words not included in the vocabulary\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))\n",
    "        lengths.append(len(sentence_tokenized))\n",
    "    return np.quantile(lengths, [.5,.75,.9,.95,.99,.995,1])\n",
    "\n",
    "# uncomment if u wanna show quantiles of the sentence lengths\n",
    "#get_sentence_lengths_quantiles(capIdx_cap_train, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 29 # perform get_sentence_lengths_quantiles to evaluate this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(capIdx_cap, tokenizer, vocab, max_sentence_length):\n",
    "    y = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # tokenizes the sentence and put words not in vocab to None\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))[:max_sentence_length] # filter None (not in vocab) from sentence and slice to max_sentence_length\n",
    "        max_length_diff = max_sentence_length - len(sentence_tokenized) + 1 # calculates the numbers of '<eos>', +1 is to ensure that every sentence has at least 1 '<eos>'\n",
    "        sentence_tokenized = np.array(sentence_tokenized+[vocab['<eos>']]*max_length_diff)\n",
    "        y.append(sentence_tokenized)\n",
    "        \n",
    "    y = np.array(y)\n",
    "    \n",
    "    return torch.from_numpy(y)\n",
    "    \n",
    "y_train = process_sentences(capIdx_cap_train, tokenizer, vocab, max_sentence_length)\n",
    "y_val = process_sentences(capIdx_cap_val, tokenizer, vocab, max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning(Dataset):\n",
    "    def __init__(self, get_img_path, preprocess, y):\n",
    "        self.get_img_path = get_img_path\n",
    "        self.preprocess = preprocess\n",
    "        self.y = y.long()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.preprocess(Image.open(self.get_img_path(idx)))\n",
    "        cap = self.y[idx]\n",
    "        return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ImageCaptioning(get_img_path_train, preprocess, y_train)\n",
    "data_val = ImageCaptioning(get_img_path_val, preprocess, y_val)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgCapEncoderCNN(nn.Module):\n",
    "    \"\"\"Pretrained ResNet18 with removed last .fc layer\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ImgCapEncoderCNN, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1] # deletes the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
    "        self.bn = nn.BatchNorm1d(embedding_dim, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            out = self.resnet(images)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.bn(self.linear(out))\n",
    "        return out\n",
    "\n",
    "class ImgCapDecoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM Decoder with built-in Word Embedding\"\"\"\n",
    "    def __init__(self, embedding_dim, vocab_size, max_sentence_length):\n",
    "        # embedding_dim=128, vocab_size=10000, max_sentence_length=29\n",
    "        super(ImgCapDecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.inv_embedding = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sentence_length = max_sentence_length + 1 # +1 to cover the added <eos> at the end of every sentence\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"`y` is to provide the true label during training\"\"\"\n",
    "        out = []\n",
    "        hidden = None\n",
    "        for i in range(self.max_sentence_length):\n",
    "            output, hidden = self.lstm(x.unsqueeze(0), hidden)\n",
    "            y_pred = self.inv_embedding(output)\n",
    "            if y == None:\n",
    "                _, predicted = y_pred.max(1)\n",
    "                out.append(predicted)\n",
    "            else:\n",
    "                predicted = y[:,i] # takes the true word as input for training\n",
    "                out.append(y_pred)\n",
    "            x = self.embedding(predicted)\n",
    "        out = torch.stack(out, 1)[0]\n",
    "        return out\n",
    "    \n",
    "class ImgCapNet(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, max_sentence_length):\n",
    "        super(ImgCapNet, self).__init__()\n",
    "        # maybe implement factory pattern to get models with stored parameters\n",
    "        self.ImgCapEncoderCNN = ImgCapEncoderCNN(embedding_dim)\n",
    "        self.ImgCapDecoderLSTM = ImgCapDecoderLSTM(embedding_dim, vocab_size, max_sentence_length)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        x1 = self.ImgCapEncoderCNN(x)\n",
    "        x2 = self.ImgCapDecoderLSTM(x1, y)\n",
    "        return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_iter(json_):\n",
    "    with open('iter.json', 'w') as fp:\n",
    "        json.dump(json_, fp)\n",
    "        \n",
    "def get_iter():\n",
    "    path = 'iter.json'\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as fp:\n",
    "            json_ = json.load(fp)\n",
    "    else:\n",
    "        json_ = dict()\n",
    "    return json_\n",
    "\n",
    "def get_iter_value(json_, key):\n",
    "    if key not in json_:\n",
    "        json_[key] = 0\n",
    "    return json_[key]\n",
    "\n",
    "def save_model(model, model_num):\n",
    "    path = './models/model_'+str(model_num)+'.pth'\n",
    "    torch.save(model.state_dict(), 'model_weights.pth')\n",
    "    \n",
    "def get_model(model, model_num):\n",
    "    \"\"\"Loads the stored weights if it exists\"\"\"\n",
    "    path = './models/model_'+str(model_num)+'.pth'\n",
    "    if os.path.exists(path):\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data_class, batch_size, lr, n_epochs, model_num):\n",
    "    icn = ImgCapNet(128, len(vocab), max_sentence_length)\n",
    "    icn = get_model(icn, model_num).to(device)\n",
    "    dataloader = DataLoader(data_class, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(icn.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    iter_name = 'iter_'+str(model_num)\n",
    "    iter_name_batch = 'iter_batch_'+str(model_num)\n",
    "    iter_json = get_iter()\n",
    "    n_iter = get_iter_value(iter_json, iter_name)\n",
    "    n_iter_batch = get_iter_value(iter_json, iter_name_batch)\n",
    "    writer = SummaryWriter('./runs/model_'+str(model_num))\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss = 0\n",
    "            for i, (imgs, caps) in enumerate(dataloader):\n",
    "                raise NotImplementedError()\n",
    "                icn.train()\n",
    "                imgs = imgs.to(device)\n",
    "                caps = caps.to(device)\n",
    "                logits = icn(imgs, caps)\n",
    "                logits = logits.view(logits.shape[0]*logits.shape[1], len(vocab)) # reorders the labels so, that each vocab char of first_sentence is in a row, followed by second_sentence, etc.\n",
    "                targets = caps.ravel() # orders the captions to that first_sentence, second_sentence, etc... for `nn.CrossEntropyLoss`\n",
    "                loss = loss_fn(logits, targets)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    minibatch_ratio = imgs.shape[0] / dataloader.batch_size\n",
    "                    train_loss += loss.item() * minibatch_ratio\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                writer.add_scalar('batch_loss', loss, n_iter_batch) # track minibatch_loss\n",
    "                n_iter_batch += 1\n",
    "                iter_json[iter_name_batch] = n_iter_batch\n",
    "\n",
    "            \n",
    "            train_loss /= len(dataloader)\n",
    "            writer.add_scalar('train_loss', train_loss, n_iter)\n",
    "            n_iter += 1\n",
    "            iter_json[iter_name] = n_iter\n",
    "            \n",
    "        # stores `iter_json` & `icn` model when either all epochs are through or a KeyboardInterrupt occured\n",
    "        save_iter(iter_json)\n",
    "        save_model(icn, model_num)\n",
    "    except KeyboardInterrupt:\n",
    "        save_iter(iter_json)\n",
    "        save_model(icn, model_num)\n",
    "            \n",
    "train_loop(data_class=data_train, batch_size=64, lr=.001, n_epochs=3, model_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, ImgCapNet, device, writer):\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X, y in dataloader:\n",
    "        model.train()\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        logits = model(X, y)\n",
    "        loss = nn.CrossEntropyLoss(logits, y)\n",
    "            \n",
    "        # Implements l1_regularization (l1 after Compute Accuracy to not have regularization term in loss observation on TensorBoard)\n",
    "        if reg == 'l1':\n",
    "            l1_reg = torch.tensor(0.).to(device)\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    l1_reg += torch.norm(param, 1)\n",
    "            loss += (reg_param / X.shape[0]) * l1_reg\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            minibatch_ratio = X.shape[0] / dataloader.batch_size # is always 1, except for the last mini-batch to adjust loss weight\n",
    "            train_loss += loss.item() * minibatch_ratio # * minibatch_ratio because the `loss_fn` is set to 'mean'\n",
    "            pred_classes = logits.argmax(1)\n",
    "            correct += (pred_classes == y).type(torch.float).sum().item()\n",
    "            with warnings.catch_warnings(): # catches \"y_pred contains classes not in y_true\"\n",
    "                warnings.filterwarnings('ignore', category=UserWarning)\n",
    "                bas += balanced_accuracy_score(y.cpu(), pred_classes.cpu()) * minibatch_ratio # upscaling because the last mini-batch is likely to have a different size\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(dataloader) # scale test loss to adjust summation in mini-batch loop\n",
    "    correct /= len(dataloader.dataset)\n",
    "    bas /= len(dataloader)\n",
    "    writer.add_scalar('model_'+str(n_model)+'/loss_train', train_loss, n_iter)\n",
    "    writer.add_scalar('model_'+str(n_model)+'/accuracy_train', correct, n_iter)\n",
    "    writer.add_scalar('model_'+str(n_model)+'/balanced_accuracy_train', bas, n_iter)\n",
    "    return train_loss, correct, bas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.05.2021: 2h --> Sprechstunde, Challenge & Beschreibung anschauen<br>\n",
    "28.05.2021: 1h --> Download und Integration der Daten und API<br>\n",
    "01.06.2021: 5h 30min --> API vertraut machen, Daten laden<br>\n",
    "02.06.2021: 6h 45min --> Standardisierung der Channels, Laden der Bilder<br>\n",
    "03.06.2021: 7h 15min --> Erstellen des Vocabulars<br>\n",
    "04.06.2021: 5h 30min --> Vocabular und Preprocessing von captions fertiggestellt<br>\n",
    "05.06.2021: 4h --> bugfixing & `get_X_idx` validierung, ImgCapNet \\_\\_init\\_\\_ erstellt<br>\n",
    "06.06.2021: 2h --> update ImgCapNet Architektur<br>\n",
    "14.06.2021: 8h --> update der Architektur<br>\n",
    "\n",
    "Sum: 42h 00min"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
