{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of 39,181 images originating from people who are blind that are each paired with 5 captions. Our proposed challenge addresses the task of predicting a suitable caption given an image. Ultimately, we hope this work will educate more people about the technological needs of blind people while providing an exciting new opportunity for researchers to develop assistive technologies that eliminate accessibility barriers for blind people (https://vizwiz.org/tasks-and-datasets/image-captioning/).\n",
    "\n",
    "The goal of this Challenge is to create a single model similar to https://arxiv.org/pdf/1411.4555.pdf to get reasonable results on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#def extract_zip(filename):\n",
    "#    with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#        zip_ref.extractall('./')\n",
    "#            \n",
    "#extract_zip('vizwiz_eval_cap.zip')\n",
    "\n",
    "#!pip3 install torch torchvision torchaudio\n",
    "#!pip install tensorboard\n",
    "#!pip install torchtext\n",
    "#!pip install -U spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizwiz_api.vizwiz import VizWiz\n",
    "from vizwiz_eval_cap.eval import VizWizEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms, models\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import Counter\n",
    "\n",
    "import json, os, re, time, sys\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_alloc_dicts(set_name, vizwiz=None):\n",
    "    # be sure if `vizwiz` is set, that it contains the `set_name` dataset\n",
    "    if (set_name != 'train') and (set_name != 'val') and (set_name != 'test'):\n",
    "        raise Exception('only \"train\", \"val\" or \"test\" is a valid `set_name`')\n",
    "    \n",
    "    if not isinstance(vizwiz, VizWiz):\n",
    "        ann_path = './annotations/'+set_name+'.json'\n",
    "        vizwiz = VizWiz(ann_path, ignore_rejected=True, ignore_precanned=True)\n",
    "    \n",
    "    img_path_prefix = './images/'+set_name+'/'\n",
    "    img_path_preproc_prefix = './images_preproc/'+set_name+'/'\n",
    "    img_ids_anns = np.unique([vizwiz.anns[i]['image_id'] for i in vizwiz.anns])\n",
    "    img_ids_imgs = np.unique([vizwiz.imgs[i]['id'] for i in vizwiz.imgs])\n",
    "    img_ids_with_capitions = np.array([_id for _id in img_ids_imgs if _id in img_ids_anns])\n",
    "    imgIdx_enumIdx = {imgIdx:idx for idx, imgIdx in enumerate(img_ids_with_capitions)}\n",
    "    enumIdx_imgIdx = {idx:imgIdx for idx, imgIdx in enumerate(img_ids_with_capitions)}\n",
    "    \n",
    "    imgIdx_imgPath = {vizwiz.imgs[i]['id']:img_path_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs if vizwiz.imgs[i]['id'] in img_ids_with_capitions}\n",
    "    imgIdx_imgPath_preproc = {vizwiz.imgs[i]['id']:img_path_preproc_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs if vizwiz.imgs[i]['id'] in img_ids_with_capitions}\n",
    "    capIdx_imgIdx = {vizwiz.anns[i]['id']:vizwiz.anns[i]['image_id'] for i in vizwiz.anns}\n",
    "    enumIdx_capIdx = {idx:vizwiz.anns[i]['id'] for idx, i in enumerate(vizwiz.anns)}\n",
    "    capIdx_cap = {vizwiz.anns[i]['id']:vizwiz.anns[i]['caption'] for i in vizwiz.anns}\n",
    "    \n",
    "    def get_img_preproc_path(idx):\n",
    "        capIdx = enumIdx_capIdx[idx]\n",
    "        imgIdx = capIdx_imgIdx[capIdx]\n",
    "        imgPath = imgIdx_imgPath_preproc[imgIdx]\n",
    "        return imgPath\n",
    "    \n",
    "    def get_X_idx(idx):\n",
    "        capIdx = enumIdx_capIdx[idx]\n",
    "        imgIdx = capIdx_imgIdx[capIdx]\n",
    "        X_idx = imgIdx_enumIdx[imgIdx]\n",
    "        return X_idx\n",
    "        \n",
    "    return imgIdx_imgPath, imgIdx_imgPath_preproc, capIdx_imgIdx, enumIdx_capIdx, capIdx_cap, enumIdx_imgIdx, get_img_preproc_path, get_X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train = './annotations/train.json'\n",
    "vizwiz_train = VizWiz(ann_train, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_train, imgIdx_imgPath_preproc_train, capIdx_imgIdx_train, enumIdx_capIdx_train, \\\n",
    "capIdx_cap_train, enumIdx_imgIdx_train, get_img_preproc_path_train, get_X_idx_train = get_alloc_dicts('train', vizwiz_train)\n",
    "\n",
    "ann_val = './annotations/val.json'\n",
    "vizwiz_val = VizWiz(ann_val, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_val, imgIdx_imgPath_preproc_val, capIdx_imgIdx_val, enumIdx_capIdx_val, \\\n",
    "capIdx_cap_val, enumIdx_imgIdx_val, get_img_preproc_path_val, get_X_idx_val = get_alloc_dicts('val', vizwiz_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "to_pil = transforms.Compose([transforms.ToPILImage()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_imgs(imgIdx_imgPath, imgIdx_imgPath_preproc, preprocess):\n",
    "    \"\"\"Preprocesses all images from ./images and stores them in ./images_preproc\"\"\"\n",
    "    if not os.path.isdir('./images_preproc'):\n",
    "        os.mkdir('./images_preproc')\n",
    "    if not os.path.isdir('./images_preproc/train'):\n",
    "        os.mkdir('./images_preproc/train')\n",
    "    if not os.path.isdir('./images_preproc/val'):\n",
    "        os.mkdir('./images_preproc/val')\n",
    "    if not os.path.isdir('./images_preproc/test'):\n",
    "        os.mkdir('./images_preproc/test')\n",
    "    \n",
    "    for i in imgIdx_imgPath:\n",
    "        img = preprocess(Image.open(imgIdx_imgPath[i]))\n",
    "        torch.save(img, imgIdx_imgPath_preproc[i])\n",
    "        \n",
    "# uncomment if a set is not preprocessed yet\n",
    "#preprocess_imgs(imgIdx_imgPath_train, imgIdx_imgPath_preproc_train, preprocess)\n",
    "#preprocess_imgs(imgIdx_imgPath_val, imgIdx_imgPath_preproc_val, preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(imgIdx_imgPath_preproc):\n",
    "    \"\"\"Loads all images in memory for eager loading setup instead of lazy,\n",
    "    it assumes that `preprocess_imgs` is already done with the set u want to load\"\"\"\n",
    "    imgs = []\n",
    "    for i in imgIdx_imgPath_preproc:\n",
    "        img = torch.load(imgIdx_imgPath_preproc[i])\n",
    "        imgs.append(img)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1_idx = enumIdx_capIdx_train[0]\n",
    "cap1 = capIdx_cap_train[cap1_idx]\n",
    "print('caption:', cap1)\n",
    "img1_idx = capIdx_imgIdx_train[cap1_idx]\n",
    "img1 = Image.open(imgIdx_imgPath_train[img1_idx])\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_processed = preprocess(img1)\n",
    "img1_processed_pil = to_pil(img1_processed)\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1_processed_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_special_chars = lambda s: re.sub(r'[^a-zA-Z0-9 ]', '', s)\n",
    "\n",
    "def build_vocab(capIdx_cap_train, size=10000):\n",
    "    size -= 3 # -3 to address specials\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "    counter = Counter()\n",
    "    for i in capIdx_cap_train:\n",
    "        sentence = capIdx_cap_train[i]\n",
    "        sentence = rm_special_chars(sentence)\n",
    "        counter.update(tokenizer(sentence.lower()))\n",
    "\n",
    "    top_tokens = list(dict(counter.most_common(size)).keys())\n",
    "    drop_tokens = list(set(counter.keys()) - set(top_tokens))\n",
    "    for drop_token in drop_tokens: # drops all `drop_tokens` from `counter`\n",
    "        counter.pop(drop_token)\n",
    "\n",
    "    vocab = Vocab(counter, specials=['<bos>','<eos>','<pad>'])\n",
    "    \n",
    "    return tokenizer, vocab\n",
    "\n",
    "tokenizer, vocab = build_vocab(capIdx_cap_train, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lengths_quantiles(capIdx_cap, tokenizer, vocab):\n",
    "    lengths = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        sentence = rm_special_chars(sentence)\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # filters the words not included in the vocabulary\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))\n",
    "        lengths.append(len(sentence_tokenized))\n",
    "    return np.quantile(lengths, [.5,.75,.9,.95,.99,.995,1])\n",
    "\n",
    "# uncomment if u wanna show quantiles of the sentence lengths\n",
    "get_sentence_lengths_quantiles(capIdx_cap_train, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(capIdx_cap, tokenizer, vocab, max_sentence_length):\n",
    "    y = []\n",
    "    cap_lengths = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        sentence = rm_special_chars(sentence)\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # tokenizes the sentence and put words not in vocab to None\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))[:max_sentence_length] # filter None (not in vocab) from sentence and slice to max_sentence_length\n",
    "        cap_length = len(sentence_tokenized)\n",
    "        cap_lengths.append(cap_length + 2) # +2 is to address '<bos>' and '<eos>'\n",
    "        max_length_diff = max_sentence_length - cap_length # calculates the numbers of '<pad>' at the start of the sentence\n",
    "        sentence_tokenized = np.array([vocab['<bos>']]+sentence_tokenized+[vocab['<eos>']]+[vocab['<pad>']]*max_length_diff)\n",
    "        y.append(sentence_tokenized)\n",
    "        \n",
    "    y = np.array(y)\n",
    "    \n",
    "    return torch.from_numpy(y), cap_lengths\n",
    "    \n",
    "y_train, train_lengths = process_sentences(capIdx_cap_train, tokenizer, vocab, max_sentence_length)\n",
    "y_val, val_lengths = process_sentences(capIdx_cap_val, tokenizer, vocab, max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning(Dataset):\n",
    "    def __init__(self, get_img_fun, preprocess, y, cap_lengths, load_imgs=None, imgIdx_imgPath_preproc=None):\n",
    "        self.get_img_fun = get_img_fun\n",
    "        self.get_img_fun_name = get_img_fun.__name__\n",
    "        self.preprocess = preprocess\n",
    "        self.y = y.long()\n",
    "        self.cap_lengths = cap_lengths\n",
    "        if self.get_img_fun_name == 'get_X_idx':\n",
    "            self.X = load_imgs(imgIdx_imgPath_preproc)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.get_img_fun_name == 'get_img_preproc_path': # lazy loader\n",
    "            img = torch.load(self.get_img_fun(idx))\n",
    "        elif self.get_img_fun_name == 'get_X_idx': # eager loader\n",
    "            img = self.X[self.get_img_fun(idx)]\n",
    "        cap = self.y[idx]\n",
    "        cap_length = self.cap_lengths[idx]\n",
    "        return img, cap, cap_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ImageCaptioning(get_img_preproc_path_train, preprocess, y_train, train_lengths)\n",
    "data_val = ImageCaptioning(get_img_preproc_path_val, preprocess, y_val, val_lengths)\n",
    "\n",
    "# uncomment if eager instead of lazy img-loading is desired\n",
    "#data_train = ImageCaptioning(get_X_idx_train, preprocess, y_train, train_lengths, load_imgs, imgIdx_imgPath_preproc_train)\n",
    "#data_val = ImageCaptioning(get_X_idx_val, preprocess, y_val, val_lengths, load_imgs, imgIdx_imgPath_preproc_val)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgCapEncoderCNN(nn.Module):\n",
    "    \"\"\"Pretrained ResNet18 with altered .fc layer\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(ImgCapEncoderCNN, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1] # deletes the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embedding_dim)\n",
    "        self.bn = nn.BatchNorm1d(embedding_dim, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            out = self.resnet(images)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.bn(self.linear(out))\n",
    "        return out\n",
    "\n",
    "class ImgCapDecoderLSTM(nn.Module):\n",
    "    \"\"\"LSTM Decoder with built-in Word Embedding\"\"\"\n",
    "    def __init__(self, embedding_dim, vocab_size, max_sentence_length):\n",
    "        # embedding_dim=128, vocab_size=10000, max_sentence_length=20\n",
    "        super(ImgCapDecoderLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.inv_embedding = nn.Linear(embedding_dim, vocab_size)\n",
    "        self.lstm = nn.LSTM(embedding_dim, embedding_dim)\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sentence_length = max_sentence_length + 2 # +2 to cover the added <bos> and <eos>\n",
    "    \n",
    "    def forward(self, x, y=None, mode='pred'):\n",
    "        \"\"\"`y` is to provide the true label during training\"\"\"\n",
    "        out = []\n",
    "        hidden = None\n",
    "        for i in range(self.max_sentence_length):\n",
    "            output, hidden = self.lstm(x.unsqueeze(0), hidden)\n",
    "            output = output.squeeze(0)\n",
    "            y_pred = self.inv_embedding(output)\n",
    "            if mode == 'pred':\n",
    "                _, predicted = y_pred.max(1)\n",
    "                out.append(predicted)\n",
    "            elif mode == 'train':\n",
    "                predicted = y[:,i] # takes the true word as input for training\n",
    "                out.append(y_pred)\n",
    "            elif mode == 'test':\n",
    "                _, predicted = y_pred.max(1)\n",
    "                out.append(y_pred)\n",
    "            x = self.embedding(predicted)\n",
    "        out = torch.stack(out, 1)\n",
    "        return out\n",
    "    \n",
    "class ImgCapNet(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, max_sentence_length):\n",
    "        super(ImgCapNet, self).__init__()\n",
    "        self.ImgCapEncoderCNN = ImgCapEncoderCNN(embedding_dim)\n",
    "        self.ImgCapDecoderLSTM = ImgCapDecoderLSTM(embedding_dim, vocab_size, max_sentence_length)\n",
    "        \n",
    "    def forward(self, x, y=None, cap_lens=None, mode='pred'):\n",
    "        x1 = self.ImgCapEncoderCNN(x)\n",
    "        if (mode == 'train') or (mode == 'test'):\n",
    "            cap_lens, sort_ind = cap_lens.sort(dim=0, descending=True)\n",
    "            x1 = x1[sort_ind]\n",
    "            if mode == 'train':\n",
    "                y = y[sort_ind]\n",
    "        x2 = self.ImgCapDecoderLSTM(x1, y, mode)\n",
    "        \n",
    "        if (mode == 'train') or (mode == 'test'):\n",
    "            return x2, cap_lens.tolist(), sort_ind\n",
    "        elif mode == 'pred':\n",
    "            return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_iter(json_):\n",
    "    with open('iter.json', 'w') as fp:\n",
    "        json.dump(json_, fp)\n",
    "        \n",
    "def get_iter():\n",
    "    path = 'iter.json'\n",
    "    if os.path.exists(path):\n",
    "        with open(path, 'r') as fp:\n",
    "            json_ = json.load(fp)\n",
    "    else:\n",
    "        json_ = dict()\n",
    "    return json_\n",
    "\n",
    "def get_iter_value(json_, key, model_name):\n",
    "    if (key not in json_) or (not os.path.exists('./models/'+model_name+'.pth')):\n",
    "        json_[key] = 0\n",
    "    return json_[key]\n",
    "\n",
    "def save_model(model, model_name, iter_json, force=False, test_loss=None):\n",
    "    \"\"\"Saves the `model` if `force` is set to True or if `test_loss` is the lowest of `model_name`\"\"\"\n",
    "    path = './models/'+model_name+'.pth'\n",
    "    if force:\n",
    "        torch.save(model.state_dict(), path)\n",
    "    else:\n",
    "        key = 'test_loss_'+model_name\n",
    "        if (key not in iter_json) or (not os.path.exists('./models/'+model_name+'.pth')):\n",
    "            torch.save(model.state_dict(), path)\n",
    "            iter_json[key] = test_loss\n",
    "            save_iter(iter_json)\n",
    "        else:\n",
    "            best_test_loss = iter_json[key]\n",
    "            #if test_loss < best_test_loss: # stores model only if test_loss is the lowest\n",
    "            torch.save(model.state_dict(), path)\n",
    "            iter_json[key] = test_loss\n",
    "            save_iter(iter_json)\n",
    "    \n",
    "def get_model(model, model_num):\n",
    "    \"\"\"Loads the stored weights if it exists\"\"\"\n",
    "    path = './models/model_'+str(model_num)+'.pth'\n",
    "    if os.path.exists(path):\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_train, data_test, batch_size, lr, n_epochs, model_num, force_save_by_interrupt=True):\n",
    "    model_name = 'model_'+str(model_num)\n",
    "    iter_name = 'iter_'+str(model_num)\n",
    "    iter_name_test = 'iter_test_'+str(model_num)\n",
    "    iter_name_batch = 'iter_batch_'+str(model_num)\n",
    "    \n",
    "    dataloader = DataLoader(data_train, batch_size=batch_size, shuffle=True)\n",
    "    dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    iter_json = get_iter()\n",
    "    n_iter = get_iter_value(iter_json, iter_name, model_name)\n",
    "    n_iter_test = get_iter_value(iter_json, iter_name_test, model_name)\n",
    "    n_iter_batch = get_iter_value(iter_json, iter_name_batch, model_name)\n",
    "    writer = SummaryWriter('./runs/'+model_name)\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, test_loss = 0, 0\n",
    "            # train_loops\n",
    "            model.train()\n",
    "            for imgs, caps, caplens in dataloader:\n",
    "                imgs = imgs.to(device)\n",
    "                caps = caps.to(device)\n",
    "                caplens = caplens.to(device)\n",
    "                \n",
    "                logits, lengths, sort_ind = model(imgs, y=caps, cap_lens=caplens, mode='train')\n",
    "                targets = caps[sort_ind]\n",
    "                # pack_padded_sequence is to not take <pad> into account during training, otherwise the network would learn to add too many <pad> to the output\n",
    "                scores = pack_padded_sequence(logits, lengths, batch_first=True)[0]\n",
    "                targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "                loss = loss_fn(scores, targets)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    batch_loss = loss.item()\n",
    "                    writer.add_scalar('batch_loss', batch_loss, n_iter_batch) # track minibatch_loss\n",
    "                    n_iter_batch += 1\n",
    "                    iter_json[iter_name_batch] = n_iter_batch\n",
    "                    minibatch_ratio = imgs.shape[0] / dataloader.batch_size # correction of contribution of batch_loss to train_loss of the last batch\n",
    "                    train_loss += batch_loss * minibatch_ratio\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            train_loss /= len(dataloader)\n",
    "            writer.add_scalar('train_loss', train_loss, n_iter)\n",
    "            n_iter += 1\n",
    "            iter_json[iter_name] = n_iter\n",
    "            \n",
    "            # test_loops\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for imgs, caps, caplens in dataloader_test:\n",
    "                    imgs = imgs.to(device)\n",
    "                    caps = caps.to(device)\n",
    "                    caplens = caplens.to(device)\n",
    "                    \n",
    "                    logits, lengths, sort_ind = model(imgs, cap_lens=caplens, mode='test')\n",
    "                    targets = caps[sort_ind]\n",
    "                    # pack_padded_sequence is to not take <pad> into account during training, otherwise the network would learn to add too many <pad> to the output\n",
    "                    scores = pack_padded_sequence(logits, lengths, batch_first=True)[0]\n",
    "                    targets = pack_padded_sequence(targets, lengths, batch_first=True)[0]\n",
    "                    loss = loss_fn(scores, targets)\n",
    "                    \n",
    "                    minibatch_ratio = imgs.shape[0] / dataloader_test.batch_size\n",
    "                    test_loss += loss.item() * minibatch_ratio\n",
    "                    \n",
    "                test_loss /= len(dataloader_test)\n",
    "                writer.add_scalar('test_loss', test_loss, n_iter_test) # uses n_iter_test in case the KeyboardInterrupt occurs during test_loops\n",
    "                n_iter_test += 1\n",
    "                iter_json[iter_name_test] = n_iter_test\n",
    "            \n",
    "            # stores `iter_json` & `model` after each epoch or a KeyboardInterrupt occured and `store_by_interrupt` is set to True\n",
    "            print('epoch: '+str(epoch)+'    train_loss: '+str(train_loss)+'    test_loss: '+str(test_loss), flush=True)\n",
    "            save_iter(iter_json)\n",
    "            save_model(model, model_name, iter_json, force=False, test_loss=test_loss) # model saving only if test_loss < best_test_loss\n",
    "    except KeyboardInterrupt:\n",
    "        if force_save_by_interrupt:\n",
    "            save_iter(iter_json)\n",
    "            save_model(model, model_name, iter_json, force=True)\n",
    "            print('\\nKeyboardInterrupt, iter_json and model saved')\n",
    "        else:\n",
    "            print('\\nKeyboardInterrupt, iter_json and model NOT saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_num = 1\n",
    "icn = ImgCapNet(128, len(vocab), max_sentence_length)\n",
    "icn = get_model(icn, model_num).to(device) # loads already trained parameters of the model if model_num is available\n",
    "\n",
    "#train_model(model=icn, data_train=data_train, data_test=data_val, batch_size=64, lr=.0001, n_epochs=5, model_num=model_num, force_save_by_interrupt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_img(model, enumIdx_imgIdx, imgIdx_imgPath, imgIdx_imgPath_preproc, vocab, idx):\n",
    "    get_word = lambda idx: vocab.itos[idx]\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    imgIdx = enumIdx_imgIdx[idx]\n",
    "    img_pred = torch.load(imgIdx_imgPath_preproc[imgIdx])\n",
    "    \n",
    "    pred = model(img_pred.unsqueeze(0))[0]\n",
    "    pred_sentence = [get_word(word) for word in pred]\n",
    "    pred_sentence = ' '.join(pred_sentence)\n",
    "    print(pred_sentence)\n",
    "    \n",
    "    img = Image.open(imgIdx_imgPath[imgIdx])\n",
    "    _ = plt.figure(figsize=(7,5))\n",
    "    _ = plt.imshow(img)\n",
    "\n",
    "eval_img(icn, enumIdx_imgIdx_train, imgIdx_imgPath_train, imgIdx_imgPath_preproc_train, vocab, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.05.2021: 2h --> Sprechstunde, Challenge & Beschreibung anschauen<br>\n",
    "28.05.2021: 1h --> Download und Integration der Daten und API<br>\n",
    "01.06.2021: 5h 30min --> API vertraut machen, Daten laden<br>\n",
    "02.06.2021: 6h 45min --> Standardisierung der Channels, Laden der Bilder<br>\n",
    "03.06.2021: 7h 15min --> Erstellen des Vocabulars<br>\n",
    "04.06.2021: 5h 30min --> Vocabular und Preprocessing von captions fertiggestellt<br>\n",
    "05.06.2021: 4h --> bugfixing & `get_X_idx` validierung, ImgCapNet \\_\\_init\\_\\_ erstellt<br>\n",
    "06.06.2021: 2h --> update ImgCapNet Architektur<br>\n",
    "14.06.2021: 8h --> update der Architektur<br>\n",
    "15.06.2021: 8h --> implementierung des Train-loops und Debugging<br>\n",
    "16.06.2021: 4h --> änderungen im design vom setup & implementierung des Test-loops<br>\n",
    "17.06.2021: 2h --> implementierung early stopping über `save_model` & implementierung der Speicherung der preprocessten Bildern<br>\n",
    "18.06.2021: 6h --> Testen & Verbessern/Verändern des Modells/Preprocessing<br>\n",
    "\n",
    "Sum: 62h 00min"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
