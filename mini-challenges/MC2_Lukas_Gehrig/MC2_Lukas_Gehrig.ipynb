{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of 39,181 images originating from people who are blind that are each paired with 5 captions. Our proposed challenge addresses the task of predicting a suitable caption given an image. Ultimately, we hope this work will educate more people about the technological needs of blind people while providing an exciting new opportunity for researchers to develop assistive technologies that eliminate accessibility barriers for blind people (https://vizwiz.org/tasks-and-datasets/image-captioning/).\n",
    "\n",
    "The goal of this Challenge is to create a single model similar to https://arxiv.org/pdf/1411.4555.pdf to get reasonable results on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizwiz_api.vizwiz import VizWiz\n",
    "from vizwiz_eval_cap.eval import VizWizEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy: !pip install spacy\n",
    "# install 'en_core_web_sm': !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_alloc_dicts(set_name, vizwiz=None):\n",
    "    # be sure if `vizwiz` is set, that it contains the `set_name` dataset\n",
    "    if (set_name != 'train') and (set_name != 'val') and (set_name != 'test'):\n",
    "        raise Exception('only \"train\", \"val\" or \"test\" is a valid `set_name`')\n",
    "    \n",
    "    if not isinstance(vizwiz, VizWiz):\n",
    "        ann_path = './annotations/'+set_name+'.json'\n",
    "        vizwiz = VizWiz(ann_path, ignore_rejected=True, ignore_precanned=True)\n",
    "    \n",
    "    img_path_prefix = './images/'+set_name+'/'\n",
    "    img_ids_anns = np.unique([vizwiz.anns[i]['image_id'] for i in vizwiz.anns])\n",
    "    img_ids_imgs = np.unique([vizwiz.imgs[i]['id'] for i in vizwiz.imgs])\n",
    "    img_ids_with_capitions = np.array([_id for _id in img_ids_imgs if _id in img_ids_anns])\n",
    "    imgIdx_enumIdx = {imgIdx:idx for idx, imgIdx in enumerate(img_ids_with_capitions)}\n",
    "    \n",
    "    imgIdx_imgPath = {vizwiz.imgs[i]['id']:img_path_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs if vizwiz.imgs[i]['id'] in img_ids_with_capitions}\n",
    "    capIdx_imgIdx = {vizwiz.anns[i]['id']:vizwiz.anns[i]['image_id'] for i in vizwiz.anns}\n",
    "    enumIdx_capIdx = {idx:vizwiz.anns[i]['id'] for idx, i in enumerate(vizwiz.anns)}\n",
    "    capIdx_cap = {vizwiz.anns[i]['id']:vizwiz.anns[i]['caption'] for i in vizwiz.anns}\n",
    "    \n",
    "    def get_X_idx(idx):\n",
    "        capIdx = enumIdx_capIdx[idx]\n",
    "        imgIdx = capIdx_imgIdx[capIdx]\n",
    "        X_idx = imgIdx_enumIdx[imgIdx]\n",
    "        return X_idx\n",
    "        \n",
    "    return imgIdx_imgPath, capIdx_imgIdx, enumIdx_capIdx, capIdx_cap, get_X_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train = './annotations/train.json'\n",
    "vizwiz_train = VizWiz(ann_train, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_train, capIdx_imgIdx_train, enumIdx_capIdx_train, capIdx_cap_train, get_X_idx_train = get_alloc_dicts('train', vizwiz_train)\n",
    "\n",
    "ann_val = './annotations/val.json'\n",
    "vizwiz_val = VizWiz(ann_val, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_val, capIdx_imgIdx_val, enumIdx_capIdx_val, capIdx_cap_val, get_X_idx_val = get_alloc_dicts('val', vizwiz_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1_idx = enumIdx_capIdx_train[0]\n",
    "cap1 = capIdx_cap_train[cap1_idx]\n",
    "print('caption:', cap1)\n",
    "img1_idx = capIdx_imgIdx_train[cap1_idx]\n",
    "img1 = Image.open(imgIdx_imgPath_train[img1_idx])\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_shape = (128,128)\n",
    "resizer = transforms.Compose([transforms.Resize(resize_shape)])\n",
    "\n",
    "img1_resized = resizer(img1)\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_channel_means_and_sigmas():\n",
    "    with open('./images/train_means_'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.npy', 'rb') as f:\n",
    "        train_channel_means = np.load(f)\n",
    "    with open('./images/train_sigmas_'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.npy', 'rb') as f:\n",
    "        train_channels_sigmas = np.load(f)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "\n",
    "def load_imgs(imgIdx_imgPath, resizer, standardized=False):\n",
    "    # loading all images will take a while & some RAM\n",
    "    if standardized:\n",
    "        train_channel_means, train_channels_sigmas = load_train_channel_means_and_sigmas()\n",
    "    imgs = {}\n",
    "    for i in imgIdx_imgPath:\n",
    "        img = Image.open(imgIdx_imgPath[i])\n",
    "        img_resized = np.asarray(resizer(img))\n",
    "        if standardized: # element wise standardization to avoid RAM issues\n",
    "            img_resized = ((img_resized - train_channel_means) / train_channels_sigmas).astype(np.float32)\n",
    "        imgs[i] = img_resized\n",
    "        del img, img_resized\n",
    "        \n",
    "    if standardized:\n",
    "        imgs_tensor = np.array(list(imgs.values()), dtype=np.float32)\n",
    "    else:\n",
    "        imgs_tensor = np.array(list(imgs.values()))\n",
    "    del imgs\n",
    "    \n",
    "    return imgs_tensor\n",
    "\n",
    "X_train = load_imgs(imgIdx_imgPath_train, resizer, standardized=True)\n",
    "#X_val = load_imgs(imgIdx_imgPath_val, resizer, standardized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_channel_means_and_sigmas(X_train):\n",
    "    \"\"\"Calculates the mean and stds of the 3 RGB channels and stores it in a .npy file\n",
    "    Performs batch-wise calculation of sum to avoid RAM issues\"\"\"\n",
    "    train_channel_means = X_train.mean(axis=(0,1,2))\n",
    "\n",
    "    std_batch_size = 1000\n",
    "    std_sum = 0\n",
    "    std_n = X_train.shape[0]*X_train.shape[1]*X_train.shape[2]\n",
    "    std_idx = np.arange(0,X_train.shape[0]+std_batch_size, std_batch_size)\n",
    "    for i in range(std_idx.shape[0]-1):\n",
    "        start_idx, end_idx = std_idx[i], std_idx[i+1]\n",
    "        std_batch = X_train[start_idx:end_idx]\n",
    "        batch_sum = np.sum((std_batch - train_channel_means)**2, axis=(0,1,2))\n",
    "        std_sum += batch_sum\n",
    "    train_channels_sigmas = np.sqrt(std_sum / std_n)\n",
    "    \n",
    "    with open('./images/train_means_'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.npy', 'wb') as f:\n",
    "        np.save(f, train_channel_means)\n",
    "    with open('./images/train_sigmas_'+str(resize_shape[0])+'x'+str(resize_shape[1])+'.npy', 'wb') as f:\n",
    "        np.save(f, train_channels_sigmas)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "        \n",
    "# uncomment for mean and std calculation porpuse\n",
    "#X_train = load_imgs(imgIdx_imgPath_train, resizer, standardized=False)\n",
    "#calc_train_channel_means_and_sigmas(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText word embedding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "def build_vocab(capIdx_cap_train, size=10000):\n",
    "    tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "    counter = Counter()\n",
    "    for i in capIdx_cap_train:\n",
    "        sentence = capIdx_cap_train[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        counter.update(tokenizer(sentence.lower()))\n",
    "\n",
    "    top_tokens = list(dict(counter.most_common(size)).keys())\n",
    "    drop_tokens = list(set(counter.keys()) - set(top_tokens))\n",
    "    for drop_token in drop_tokens: # drops all `drop_tokens` from `corpus_counter`\n",
    "        counter.pop(drop_token)\n",
    "\n",
    "    vocab = Vocab(counter, specials=['<eos>'])\n",
    "    \n",
    "    return tokenizer, vocab\n",
    "\n",
    "tokenizer, vocab = build_vocab(capIdx_cap_train, size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_lengths_quantiles(capIdx_cap, tokenizer, vocab):\n",
    "    lengths = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # filters the words not included in the vocabulary\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))\n",
    "        lengths.append(len(sentence_tokenized))\n",
    "    return np.quantile(lengths, [.5,.75,.9,.95,.99,.995,1])\n",
    "\n",
    "# uncomment if u wanna show quantiles of the sentence lengths\n",
    "#get_sentence_lengths_quantiles(capIdx_cap_train, tokenizer, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 29 # perform get_sentence_lengths_quantiles to evaluate this number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences(capIdx_cap, tokenizer, vocab, max_sentence_length):\n",
    "    y = []\n",
    "    for i in capIdx_cap:\n",
    "        sentence = capIdx_cap[i]\n",
    "        if sentence[-1] == '.':\n",
    "            sentence = sentence[:-1]\n",
    "        sentence_tokenized = [vocab[token] for token in tokenizer(sentence.lower())] # tokenizes the sentence and put words not in vocab to None\n",
    "        sentence_tokenized = list(filter(None, sentence_tokenized))[:max_sentence_length] # filter None (not in vocab) from sentence and slice to max_sentence_length\n",
    "        max_length_diff = max_sentence_length - len(sentence_tokenized) + 1 # calculates the numbers of '<eos>', +1 is to ensure that every sentence has at least 1 '<eos>'\n",
    "        sentence_tokenized = sentence_tokenized+[vocab['<eos>']]*max_length_diff\n",
    "        sentence_tensor = torch.tensor(sentence_tokenized)\n",
    "        y.append(sentence_tensor)\n",
    "        \n",
    "    return y\n",
    "    \n",
    "y_train = process_sentences(capIdx_cap_train, tokenizer, vocab, max_sentence_length)\n",
    "y_val = process_sentences(capIdx_cap_val, tokenizer, vocab, max_sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning(Dataset):\n",
    "    def __init__(self, X, y, get_X_idx):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = y\n",
    "        self.get_X_idx = get_X_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgIdx = self.get_X_idx(idx)\n",
    "        img = self.X[imgIdx]\n",
    "        cap = self.y[idx]\n",
    "        return img, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = ImageCaptioning(X_train, y_train, get_X_idx_train)\n",
    "#data_val = ImageCaptioning(X_val, y_val, get_X_idx_val)\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=64, shuffle=True)\n",
    "#val_loader = DataLoader(data_val, batch_size=64, shuffle=True)\n",
    "\n",
    "out_size = lambda n, p, f, s: (n+2*p-f)/s+1 #n:img_size, p:padding, f:filter/kernel, s:stride\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgCapNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # case 128 x 128\n",
    "        self.conv1 = nn.Conv2d(3, 8, 5) # 124 x 124 x 8\n",
    "        self.pool = nn.MaxPool2d(2) # height & width / 2, depth is the same\n",
    "        self.conv2 = nn.Conv2d(8, 12, 4) # 59 x 59 x 12\n",
    "        self.conv3 = nn.Conv2d(12, 20, 3) # 28 x 28 x 20\n",
    "        self.linear = nn.Linear(14 * 14 * 20, 128)\n",
    "        \n",
    "        self.embedding = nn.Embedding(10000, 128) # input is given by a list of indices\n",
    "        self.lstm = nn.LSTM(300, 300-128)\n",
    "        self.output = nn.Linear(300-128, 10000)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.argmax = nn.argmax\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # still toDo here\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.05.2021: 2h --> Sprechstunde, Challenge & Beschreibung anschauen<br>\n",
    "28.05.2021: 1h --> Download und Integration der Daten und API<br>\n",
    "01.06.2021: 5h 30min --> API vertraut machen, Daten laden<br>\n",
    "02.06.2021: 6h 45min --> Standardisierung der Channels, Laden der Bilder<br>\n",
    "03.06.2021: 7h 15min --> Erstellen des Vocabulars<br>\n",
    "04.06.2021: 5h 30min --> Vocabular und Preprocessing von captions fertiggestellt<br>\n",
    "05.06.2021: 4h --> bugfixing & `get_X_idx` validierung, ImgCapNet \\_\\_init\\_\\_ erstellt<br>\n",
    "\n",
    "Sum: 32h 00min"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
