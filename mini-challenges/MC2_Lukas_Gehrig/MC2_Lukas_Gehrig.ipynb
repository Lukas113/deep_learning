{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of 39,181 images originating from people who are blind that are each paired with 5 captions. Our proposed challenge addresses the task of predicting a suitable caption given an image. Ultimately, we hope this work will educate more people about the technological needs of blind people while providing an exciting new opportunity for researchers to develop assistive technologies that eliminate accessibility barriers for blind people (https://vizwiz.org/tasks-and-datasets/image-captioning/).\n",
    "\n",
    "The goal of this Challenge is to create a single model similar to https://arxiv.org/pdf/1411.4555.pdf to get reasonable results on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizwiz_api.vizwiz import VizWiz\n",
    "from vizwiz_eval_cap.eval import VizWizEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_alloc_dicts(set_name, vizwiz=None):\n",
    "    # be sure if `vizwiz` is set, that it contains the `set_name` dataset\n",
    "    if (set_name != 'train') and (set_name != 'val') and (set_name != 'test'):\n",
    "        raise Exception('only \"train\", \"val\" or \"test\" is a valid `set_name`')\n",
    "    \n",
    "    if not isinstance(vizwiz, VizWiz):\n",
    "        ann_path = './annotations/'+set_name+'.json'\n",
    "        vizwiz = VizWiz(ann_path, ignore_rejected=True, ignore_precanned=True)\n",
    "    \n",
    "    img_path_prefix = './images/'+set_name+'/'\n",
    "    imgIdx_imgPath = {vizwiz.imgs[i]['id']:img_path_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs}\n",
    "    capIdx_imgIdx = {vizwiz.anns[i]['id']:vizwiz.anns[i]['image_id'] for i in vizwiz.anns}\n",
    "    enumIdx_capIdx = {idx:vizwiz.anns[i]['id'] for idx, i in enumerate(vizwiz.anns)}\n",
    "    capIdx_cap = {vizwiz.anns[i]['id']:vizwiz.anns[i]['caption'] for i in vizwiz.anns}\n",
    "        \n",
    "    return imgIdx_imgPath, capIdx_imgIdx, enumIdx_capIdx, capIdx_cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train = './annotations/train.json'\n",
    "vizwiz_train = VizWiz(ann_train, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_train, capIdx_imgIdx_train, enumIdx_capIdx_train, capIdx_cap_train = get_alloc_dicts('train', vizwiz_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1_idx = enumIdx_capIdx_train[0]\n",
    "cap1 = capIdx_cap_train[cap1_idx]\n",
    "print('caption:', cap1)\n",
    "img1_idx = capIdx_imgIdx_train[cap1_idx]\n",
    "img1 = Image.open(imgIdx_imgPath_train[img1_idx])\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_shape = (256,256)\n",
    "resizer = transforms.Compose([transforms.Resize(resize_shape)])\n",
    "\n",
    "img1_resized = resizer(img1)\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_channel_means_and_sigmas():\n",
    "    with open('./images/train_means.npy', 'rb') as f:\n",
    "        train_channel_means = np.load(f)\n",
    "    with open('./images/train_sigmas.npy', 'rb') as f:\n",
    "        train_channels_sigmas = np.load(f)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "\n",
    "def load_imgs(imgIdx_imgPath, resizer, standardized=False):\n",
    "    # loading all images will take a while (needs about 4GB RAM for training set)\n",
    "    if standardized:\n",
    "        train_channel_means, train_channels_sigmas = load_train_channel_means_and_sigmas()\n",
    "    imgs = []\n",
    "    for i in imgIdx_imgPath:\n",
    "        img = Image.open(img_paths[i])\n",
    "        img_resized = np.asarray(resizer(img))\n",
    "        if standardized: # element wise standardization to avoid RAM issues\n",
    "            img_resized = (img_resized - train_channel_means) / train_channels_sigmas\n",
    "        imgs.append(img_resized)\n",
    "        \n",
    "    imgs = np.array(imgs) # convert to numpy tensor as array\n",
    "    return imgs\n",
    "        \n",
    "X_train = load_imgs(imgIdx_imgPath, resizer, standardized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_channel_means_and_sigmas(X_train):\n",
    "    \"\"\"Calculates the mean and stds of the 3 RGB channels and stores it in a .npy file\n",
    "    Performs batch-wise calculation of sum to avoid RAM issues\"\"\"\n",
    "    train_channel_means = X_train.mean(axis=(0,1,2))\n",
    "\n",
    "    std_batch_size = 1000\n",
    "    std_sum = 0\n",
    "    std_n = X_train.shape[0]*X_train.shape[1]*X_train.shape[2]\n",
    "    std_idx = np.arange(0,X_train.shape[0]+std_batch_size, std_batch_size)\n",
    "    for i in range(std_idx.shape[0]-1):\n",
    "        start_idx, end_idx = std_idx[i], std_idx[i+1]\n",
    "        std_batch = X_train[start_idx:end_idx]\n",
    "        batch_sum = np.sum((std_batch - train_channel_means)**2, axis=(0,1,2))\n",
    "        std_sum += batch_sum\n",
    "    train_channels_sigmas = np.sqrt(std_sum / std_n)\n",
    "    \n",
    "    with open('./images/train_means.npy', 'wb') as f:\n",
    "        np.save(f, train_channel_means)\n",
    "    with open('./images/train_sigmas.npy', 'wb') as f:\n",
    "        np.save(f, train_channels_sigmas)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "        \n",
    "# calc_train_channel_means_and_sigmas(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = capIdx_cap_train[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = \"\"\"data = []\n",
    "  \n",
    "# iterate through each sentence in the file\n",
    "for i in sent_tokenize(sentence):\n",
    "    temp = []\n",
    "      \n",
    "    # tokenize the sentence into words\n",
    "    for j in word_tokenize(i):\n",
    "        temp.append(j.lower())\n",
    "    data.append(temp)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long().T[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        activity = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        return activity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
