{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning\n",
    "\n",
    "Observing that people who are blind have relied on (human-based) image captioning services to learn about images they take for nearly a decade, we introduce the first image captioning dataset to represent this real use case. This new dataset, which we call VizWiz-Captions, consists of 39,181 images originating from people who are blind that are each paired with 5 captions. Our proposed challenge addresses the task of predicting a suitable caption given an image. Ultimately, we hope this work will educate more people about the technological needs of blind people while providing an exciting new opportunity for researchers to develop assistive technologies that eliminate accessibility barriers for blind people (https://vizwiz.org/tasks-and-datasets/image-captioning/).\n",
    "\n",
    "The goal of this Challenge is to create a single model similar to https://arxiv.org/pdf/1411.4555.pdf to get reasonable results on this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vizwiz_api.vizwiz import VizWiz\n",
    "from vizwiz_eval_cap.eval import VizWizEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import gensim.downloader as api\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "import json\n",
    "from jsonpath_ng import jsonpath, parse\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "google_word2vec_path = api.load('word2vec-google-news-300', return_path=True)\n",
    "print(google_word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_alloc_dicts(set_name, vizwiz=None):\n",
    "    # be sure if `vizwiz` is set, that it contains the `set_name` dataset\n",
    "    if (set_name != 'train') and (set_name != 'val') and (set_name != 'test'):\n",
    "        raise Exception('only \"train\", \"val\" or \"test\" is a valid `set_name`')\n",
    "    \n",
    "    if not isinstance(vizwiz, VizWiz):\n",
    "        ann_path = './annotations/'+set_name+'.json'\n",
    "        vizwiz = VizWiz(ann_path, ignore_rejected=True, ignore_precanned=True)\n",
    "    \n",
    "    img_path_prefix = './images/'+set_name+'/'\n",
    "    imgIdx_imgPath = {vizwiz.imgs[i]['id']:img_path_prefix+vizwiz.imgs[i]['file_name'] for i in vizwiz.imgs}\n",
    "    capIdx_imgIdx = {vizwiz.anns[i]['id']:vizwiz.anns[i]['image_id'] for i in vizwiz.anns}\n",
    "    enumIdx_capIdx = {idx:vizwiz.anns[i]['id'] for idx, i in enumerate(vizwiz.anns)}\n",
    "    capIdx_cap = {vizwiz.anns[i]['id']:vizwiz.anns[i]['caption'] for i in vizwiz.anns}\n",
    "    \n",
    "    def get_Xy_idx(idx):\n",
    "        capIdx = enumIdx_capIdx[idx] # y_idx\n",
    "        imgIdx = capIdx_imgIdx[capIdx] # X_idx\n",
    "        return imgIdx, capIdx\n",
    "        \n",
    "    return imgIdx_imgPath, capIdx_imgIdx, enumIdx_capIdx, capIdx_cap, get_Xy_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_train = './annotations/train.json'\n",
    "vizwiz_train = VizWiz(ann_train, ignore_rejected=True, ignore_precanned=True)\n",
    "imgIdx_imgPath_train, capIdx_imgIdx_train, enumIdx_capIdx_train, capIdx_cap_train, get_Xy_idx_train = get_alloc_dicts('train', vizwiz_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap1_idx = enumIdx_capIdx_train[0]\n",
    "cap1 = capIdx_cap_train[cap1_idx]\n",
    "print('caption:', cap1)\n",
    "img1_idx = capIdx_imgIdx_train[cap1_idx]\n",
    "img1 = Image.open(imgIdx_imgPath_train[img1_idx])\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_shape = (256,256)\n",
    "resizer = transforms.Compose([transforms.Resize(resize_shape)])\n",
    "\n",
    "img1_resized = resizer(img1)\n",
    "_ = plt.figure(figsize=(7,5))\n",
    "_ = plt.imshow(img1_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_channel_means_and_sigmas():\n",
    "    with open('./images/train_means.npy', 'rb') as f:\n",
    "        train_channel_means = np.load(f)\n",
    "    with open('./images/train_sigmas.npy', 'rb') as f:\n",
    "        train_channels_sigmas = np.load(f)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "\n",
    "def load_imgs(imgIdx_imgPath, resizer, standardized=False):\n",
    "    # loading all images will take a while (needs a lot of RAM, more than 10GB during `load_imgs` and about 5GB afterwards if trainset and standardized)\n",
    "    if standardized:\n",
    "        train_channel_means, train_channels_sigmas = load_train_channel_means_and_sigmas()\n",
    "    imgs = {}\n",
    "    for i in imgIdx_imgPath:\n",
    "        img = Image.open(imgIdx_imgPath[i])\n",
    "        img_resized = np.asarray(resizer(img))\n",
    "        if standardized: # element wise standardization to avoid RAM issues\n",
    "            img_resized = ((img_resized - train_channel_means) / train_channels_sigmas).astype(np.float16)\n",
    "        imgs[i] = img_resized\n",
    "        del img\n",
    "        del img_resized\n",
    "        \n",
    "    imgs_tensor = np.array(list(imgs.values()), dtype=np.float16) # convert to numpy tensor as array\n",
    "    del imgs\n",
    "    return imgs_tensor\n",
    "        \n",
    "X_train = load_imgs(imgIdx_imgPath_train, resizer, standardized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_channel_means_and_sigmas(X_train):\n",
    "    \"\"\"Calculates the mean and stds of the 3 RGB channels and stores it in a .npy file\n",
    "    Performs batch-wise calculation of sum to avoid RAM issues\"\"\"\n",
    "    train_channel_means = X_train.mean(axis=(0,1,2))\n",
    "\n",
    "    std_batch_size = 1000\n",
    "    std_sum = 0\n",
    "    std_n = X_train.shape[0]*X_train.shape[1]*X_train.shape[2]\n",
    "    std_idx = np.arange(0,X_train.shape[0]+std_batch_size, std_batch_size)\n",
    "    for i in range(std_idx.shape[0]-1):\n",
    "        start_idx, end_idx = std_idx[i], std_idx[i+1]\n",
    "        std_batch = X_train[start_idx:end_idx]\n",
    "        batch_sum = np.sum((std_batch - train_channel_means)**2, axis=(0,1,2))\n",
    "        std_sum += batch_sum\n",
    "    train_channels_sigmas = np.sqrt(std_sum / std_n)\n",
    "    \n",
    "    with open('./images/train_means.npy', 'wb') as f:\n",
    "        np.save(f, train_channel_means)\n",
    "    with open('./images/train_sigmas.npy', 'wb') as f:\n",
    "        np.save(f, train_channels_sigmas)\n",
    "    return train_channel_means, train_channels_sigmas\n",
    "        \n",
    "# calc_train_channel_means_and_sigmas(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_Word2Vec = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(google_word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = capIdx_cap_train[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = []\n",
    "  \n",
    "# tokenize the sentence into words\n",
    "for i in word_tokenize(sentence):\n",
    "    lower = i.lower()\n",
    "    #vec = google_Word2Vec.get_vector(lower)\n",
    "    tokenized_sentence.append(lower)\n",
    "\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText word embedding!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "white = google_Word2Vec.get_vector('white')\n",
    "black = google_Word2Vec.get_vector('black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_Word2Vec.similarity('bless','blessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google_Word2Vec.most_similar(positive=[black], topn=1)# wie soll das gehen? cosine similarity zu allen anderen Vektoren brauch zu viel Zeit und Speicher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptioning(Dataset):\n",
    "    def __init__(self, X, y, get_Xy_idx):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y).long().T[0]\n",
    "        self.get_Xy_idx = get_Xy_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        activity = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        return activity, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
